# SFT + LoRA configuration

model:
  base_checkpoint: google/gemma-3-270m-it
  peft_type: lora
  peft_rank: 8
  peft_alpha: 16

training:
  mode: sft
  epochs: 3
  batch_size: 4
  learning_rate: 5e-5
